<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Home on Deeptanshu Jha</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Deeptanshu Jha</description>
    <generator>Hugo -- 0.154.5</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Jan 2026 21:45:48 -0800</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>When Small Models Are Better: A Peek into Geometry of Intelligence</title>
      <link>http://localhost:1313/essays/when-small-models-are-better-geometry-of-intelligence/</link>
      <pubDate>Mon, 26 Jan 2026 21:45:48 -0800</pubDate>
      <guid>http://localhost:1313/essays/when-small-models-are-better-geometry-of-intelligence/</guid>
      <description>&lt;h3 id=&#34;tldr&#34;&gt;TL;DR&lt;/h3&gt;
&lt;p&gt;Most everyday prompts are &lt;strong&gt;local manifold walks&lt;/strong&gt;: the meaning stays in the same semantic basin, you’re just changing &lt;em&gt;shape&lt;/em&gt; (summary, tone, structure). Small models are great at this. Big models are best when the task demands &lt;strong&gt;global consistency&lt;/strong&gt; and multi-hop synthesis.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We have a &lt;strong&gt;&amp;ldquo;sledgehammer&amp;rdquo; problem&lt;/strong&gt; in AI.&lt;/p&gt;
&lt;p&gt;We are using massive, 100B+ parameter models to fix typos, format markdown, and summarize transcripts—tasks that are closer to &lt;strong&gt;doing long division with a supercomputer&lt;/strong&gt; than “needing intelligence.”&lt;/p&gt;</description>
    </item>
    <item>
      <title>Claude Code Rate Limits: Diagnosing my token usage </title>
      <link>http://localhost:1313/essays/claude-code-rate-limits-token-budgets/</link>
      <pubDate>Mon, 26 Jan 2026 21:04:07 -0800</pubDate>
      <guid>http://localhost:1313/essays/claude-code-rate-limits-token-budgets/</guid>
      <description>&lt;p&gt;Every few hours using Claude Code, I hit a wall. It is incredibly frustrating to be mid-flow and get locked out by a rate limit. After burning through roughly &lt;strong&gt;44 million tokens&lt;/strong&gt; and a &lt;strong&gt;$67.58 bill&lt;/strong&gt;, I dug into the logs to find out why.&lt;/p&gt;
&lt;p&gt;The data shows I wasn&amp;rsquo;t paying for intelligence. I was paying a &amp;ldquo;memory tax&amp;rdquo; for conversations that never ended.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-68-breakdown&#34;&gt;The $68 breakdown&lt;/h2&gt;
&lt;p&gt;In about 17 days, I made nearly 500 requests. Here is where the money actually went:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Strange Habits of LLMs: Oops I Did It Again</title>
      <link>http://localhost:1313/essays/strange-habits-of-llms/</link>
      <pubDate>Sun, 25 Jan 2026 22:48:41 -0800</pubDate>
      <guid>http://localhost:1313/essays/strange-habits-of-llms/</guid>
      <description>&lt;p&gt;I was fixing a bug for my newborn&amp;rsquo;s app and ChatGPT had a complete brain-body disconnect:&lt;/p&gt;
&lt;p&gt;It kept reasoning &amp;ldquo;don&amp;rsquo;t do the thing&amp;rdquo; → did it → &amp;ldquo;oops I did this&amp;rdquo; → redo → &amp;ldquo;oops I did it again&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In the SAME response. Five times. (See pic)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Five attempts. Same mistake. Every single time.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I couldn&amp;rsquo;t resist trying to anthropomorphize this: if this were a human, why would they do this?&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Evals: What I learned shipping production grade billing systems</title>
      <link>http://localhost:1313/essays/realwordevals/</link>
      <pubDate>Sun, 25 Jan 2026 13:45:00 -0800</pubDate>
      <guid>http://localhost:1313/essays/realwordevals/</guid>
      <description>&lt;h2 id=&#34;part-1-evals-101---what-are-evaluations&#34;&gt;Part 1: Evals 101 - What Are Evaluations?&lt;/h2&gt;
&lt;h3 id=&#34;the-core-concept&#34;&gt;The Core Concept&lt;/h3&gt;
&lt;p&gt;When you build an AI system that answers questions using documents (a RAG system), you need to know: &lt;strong&gt;does it work?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An evaluation is a systematic way to measure your system&amp;rsquo;s performance using metrics. Instead of manually checking a few examples and saying &amp;ldquo;looks good,&amp;rdquo; you:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a test dataset of questions with known correct answers&lt;/li&gt;
&lt;li&gt;Run your system on those questions&lt;/li&gt;
&lt;li&gt;Measure how often it gets the right answer&lt;/li&gt;
&lt;li&gt;Track this score over time&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Build Your Own Glean: A Production RAG System</title>
      <link>http://localhost:1313/essays/real-world-rag-partone/</link>
      <pubDate>Sun, 25 Jan 2026 13:45:00 -0800</pubDate>
      <guid>http://localhost:1313/essays/real-world-rag-partone/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve spent three years building AI tools at companies and on my own side projects. The biggest lesson: demos are easy, production is hard.&lt;/p&gt;
&lt;p&gt;This guide covers what actually matters, the parts that break in production: PDF parsing, hybrid search, permission enforcement, ranking pipelines, and evaluation frameworks that aren&amp;rsquo;t based on vibes.&lt;/p&gt;
&lt;h2 id=&#34;why-rag-exists-the-actual-reasons&#34;&gt;Why RAG Exists (the actual reasons)&lt;/h2&gt;
&lt;p&gt;Base models don&amp;rsquo;t know your internal docs. You can&amp;rsquo;t retrain GPT-4 every time someone updates a wiki page. RAG gives you three things base models don&amp;rsquo;t:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Summarizer (2014)</title>
      <link>http://localhost:1313/projects/summarizer/</link>
      <pubDate>Sun, 25 Jan 2026 00:00:00 -0800</pubDate>
      <guid>http://localhost:1313/projects/summarizer/</guid>
      <description>&lt;p&gt;Extractive text summarization in Python using classic NLP + graph ranking. It ended up powering multiple Reddit bots. Source &lt;a href=&#34;https://github.com/lekhakpadmanabh/Summarizer/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rebuilding (resuscitating) this site</title>
      <link>http://localhost:1313/essays/resuscitating-this-site/</link>
      <pubDate>Sat, 24 Jan 2026 22:10:00 -0800</pubDate>
      <guid>http://localhost:1313/essays/resuscitating-this-site/</guid>
      <description>&lt;p&gt;I logged into GitHub after 14 years and found this site sitting there like an abandoned project. Another GitHub account exists somewhere, tied to an email I&amp;rsquo;ll never check again (github.com/lekhakpadmanabh). So I picked this one, shipped something simple, and got it live.&lt;/p&gt;
&lt;p&gt;What started as &amp;ldquo;just website setup&amp;rdquo; turned into a surprisingly good session with ChatGPT and Claude Code. Same product instincts, different canvas.&lt;/p&gt;
&lt;h2 id=&#34;make-the-smallest-atomic-desire-true&#34;&gt;Make the smallest atomic desire true&lt;/h2&gt;
&lt;p&gt;I wanted two things:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
